{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcd9b3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25af4a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path=\"Dataset/en-de/\"\n",
    "en = pd.read_csv(base_path+'idiom_trainplus.en',sep='\\n', header=None,names=['en'])\n",
    "de = pd.read_csv(base_path+'idiom_trainplus.de',sep='\\n', header=None,names=['de'])\n",
    "info = pd.read_csv(base_path+'idiom_trainplus.info',sep='\\t',header=None,names=[\"spread\", \"en_idiom\", \"de_idiom\",'frequency'])\n",
    "data = pd.concat([en,de,info],axis=1)\n",
    "data.columns = ['en','de',\"spread\", \"en_idiom\", \"de_idiom\",'frequency']\n",
    "data = data.drop(['spread','frequency'],axis=1)\n",
    "data['en_task'] = 'translate to german: '+data['en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad3cc0db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>de</th>\n",
       "      <th>en_idiom</th>\n",
       "      <th>de_idiom</th>\n",
       "      <th>en_task</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Could you please pass this request on to the P...</td>\n",
       "      <td>Könnten Sie diese Frage bitte an den Ratsvorsi...</td>\n",
       "      <td>to pass on</td>\n",
       "      <td>das zeitliche segnen</td>\n",
       "      <td>translate to german: Could you please pass thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You may be forbidden to pass it on , and you a...</td>\n",
       "      <td>Es ist Ihnen vielleicht verboten , sie weiterz...</td>\n",
       "      <td>to pass on</td>\n",
       "      <td>das zeitliche segnen</td>\n",
       "      <td>translate to german: You may be forbidden to p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I should like to thank everyone who has helped...</td>\n",
       "      <td>Ich glaube , man darf hier allen , die mitgeho...</td>\n",
       "      <td>to pull together</td>\n",
       "      <td>am gleichen strang ziehen</td>\n",
       "      <td>translate to german: I should like to thank ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I will pass on your appreciation to my colleag...</td>\n",
       "      <td>Ich werde Ihre anerkennenden Worte an Gay Mitc...</td>\n",
       "      <td>to pass on</td>\n",
       "      <td>das zeitliche segnen</td>\n",
       "      <td>translate to german: I will pass on your appre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>That is why our group is calling for the Confe...</td>\n",
       "      <td>Darum fordert unsere Fraktion , dass die Entsc...</td>\n",
       "      <td>to be on hold</td>\n",
       "      <td>auf eis liegen</td>\n",
       "      <td>translate to german: That is why our group is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2297</th>\n",
       "      <td>Popularized by Joan Jett ( originally by The A...</td>\n",
       "      <td>Anfangs war ich diesem Album gegenüber etwas s...</td>\n",
       "      <td>i say !</td>\n",
       "      <td>hören sie mal !</td>\n",
       "      <td>translate to german: Popularized by Joan Jett ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2298</th>\n",
       "      <td>Today &amp;apos;s debate is , therefore , very imp...</td>\n",
       "      <td>Die heutige Debatte ist daher insofern sehr be...</td>\n",
       "      <td>to pass on</td>\n",
       "      <td>das zeitliche segnen</td>\n",
       "      <td>translate to german: Today &amp;apos;s debate is ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2299</th>\n",
       "      <td>At the end of April 2010 , they came together ...</td>\n",
       "      <td>Sie versammelten sich Ende April 2010 am Klaus...</td>\n",
       "      <td>to pass on</td>\n",
       "      <td>das zeitliche segnen</td>\n",
       "      <td>translate to german: At the end of April 2010 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2300</th>\n",
       "      <td>Our reception is open 24 hours for your conven...</td>\n",
       "      <td>In unseren Konferenzräumen finden bis zu 80 Pe...</td>\n",
       "      <td>to go places</td>\n",
       "      <td>es zu etwas bringen</td>\n",
       "      <td>translate to german: Our reception is open 24 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2301</th>\n",
       "      <td>None of us would want , in this day and age , ...</td>\n",
       "      <td>Keiner von uns will heute noch mit der Pferdek...</td>\n",
       "      <td>in this day and age</td>\n",
       "      <td>in der heutigen zeit</td>\n",
       "      <td>translate to german: None of us would want , i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2302 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     en  \\\n",
       "0     Could you please pass this request on to the P...   \n",
       "1     You may be forbidden to pass it on , and you a...   \n",
       "2     I should like to thank everyone who has helped...   \n",
       "3     I will pass on your appreciation to my colleag...   \n",
       "4     That is why our group is calling for the Confe...   \n",
       "...                                                 ...   \n",
       "2297  Popularized by Joan Jett ( originally by The A...   \n",
       "2298  Today &apos;s debate is , therefore , very imp...   \n",
       "2299  At the end of April 2010 , they came together ...   \n",
       "2300  Our reception is open 24 hours for your conven...   \n",
       "2301  None of us would want , in this day and age , ...   \n",
       "\n",
       "                                                     de             en_idiom  \\\n",
       "0     Könnten Sie diese Frage bitte an den Ratsvorsi...           to pass on   \n",
       "1     Es ist Ihnen vielleicht verboten , sie weiterz...           to pass on   \n",
       "2     Ich glaube , man darf hier allen , die mitgeho...     to pull together   \n",
       "3     Ich werde Ihre anerkennenden Worte an Gay Mitc...           to pass on   \n",
       "4     Darum fordert unsere Fraktion , dass die Entsc...        to be on hold   \n",
       "...                                                 ...                  ...   \n",
       "2297  Anfangs war ich diesem Album gegenüber etwas s...              i say !   \n",
       "2298  Die heutige Debatte ist daher insofern sehr be...           to pass on   \n",
       "2299  Sie versammelten sich Ende April 2010 am Klaus...           to pass on   \n",
       "2300  In unseren Konferenzräumen finden bis zu 80 Pe...         to go places   \n",
       "2301  Keiner von uns will heute noch mit der Pferdek...  in this day and age   \n",
       "\n",
       "                       de_idiom  \\\n",
       "0          das zeitliche segnen   \n",
       "1          das zeitliche segnen   \n",
       "2     am gleichen strang ziehen   \n",
       "3          das zeitliche segnen   \n",
       "4                auf eis liegen   \n",
       "...                         ...   \n",
       "2297            hören sie mal !   \n",
       "2298       das zeitliche segnen   \n",
       "2299       das zeitliche segnen   \n",
       "2300        es zu etwas bringen   \n",
       "2301       in der heutigen zeit   \n",
       "\n",
       "                                                en_task  \n",
       "0     translate to german: Could you please pass thi...  \n",
       "1     translate to german: You may be forbidden to p...  \n",
       "2     translate to german: I should like to thank ev...  \n",
       "3     translate to german: I will pass on your appre...  \n",
       "4     translate to german: That is why our group is ...  \n",
       "...                                                 ...  \n",
       "2297  translate to german: Popularized by Joan Jett ...  \n",
       "2298  translate to german: Today &apos;s debate is ,...  \n",
       "2299  translate to german: At the end of April 2010 ...  \n",
       "2300  translate to german: Our reception is open 24 ...  \n",
       "2301  translate to german: None of us would want , i...  \n",
       "\n",
       "[2302 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3f7729",
   "metadata": {},
   "source": [
    "### Skip preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b7ae25",
   "metadata": {},
   "source": [
    "### Load model and get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c2b4fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFT5ForConditionalGeneration, T5Tokenizer\n",
    "from tqdm import tqdm\n",
    "model = TFT5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6705fd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(iterable, n=1):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5eb284a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [17:24<00:00, 22.23s/it]\n"
     ]
    }
   ],
   "source": [
    "article = data['en_task'].to_list()\n",
    "article_batches = []\n",
    "[article_batches.append(b) for b in batch(article, 50) ]\n",
    "translation = []\n",
    "#have to use a for loop because else the kernel dies\n",
    "for art in tqdm(article_batches):\n",
    "    batch = tokenizer.prepare_seq2seq_batch(src_texts=art, return_tensors=\"tf\")\n",
    "    #print(\"batch prepare done, start generating output id's\")\n",
    "    output_ids = model.generate(input_ids=batch.input_ids, num_return_sequences=1, num_beams=8, length_penalty=0.1)\n",
    "    #print(\"output id's generated, starting decoding\")\n",
    "    translation.extend(tokenizer.batch_decode(output_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5061598c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['de_translation_t5'] = translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85b0cb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFT5ForConditionalGeneration: ['encoder.embed_tokens.weight', 'lm_head.weight', 'decoder.embed_tokens.weight']\n",
      "- This IS expected if you are initializing TFT5ForConditionalGeneration from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFT5ForConditionalGeneration from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFT5ForConditionalGeneration.from_pretrained('outputs_training/model_files/pytorch_model.bin',from_pt=True,config='outputs_training/model_files/config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02c11553",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [16:28<00:00, 21.04s/it]\n"
     ]
    }
   ],
   "source": [
    "article = data['en_task'].to_list()\n",
    "article_batches = []\n",
    "[article_batches.append(b) for b in batch(article, 50) ]\n",
    "translation = []\n",
    "#have to use a for loop because else the kernel dies\n",
    "for art in tqdm(article_batches):\n",
    "    batch = tokenizer.prepare_seq2seq_batch(src_texts=art, return_tensors=\"tf\")\n",
    "    #print(\"batch prepare done, start generating output id's\")\n",
    "    output_ids = model.generate(input_ids=batch.input_ids, num_return_sequences=1, num_beams=8, length_penalty=0.1)\n",
    "    #print(\"output id's generated, starting decoding\")\n",
    "    translation.extend(tokenizer.batch_decode(output_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc1b64a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['de_translation_t5_trained'] = translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a9e6f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('preliminary_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a6580e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Internal: C:\\projects\\sentencepiece\\src\\sentencepiece_processor.cc(891) [model_proto->ParseFromArray(serialized.data(), serialized.size())] ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8572\\2011046304.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#not sure if we even have to load in the tokenizes, is it even trained? idk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mT5Tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'outputs_training/model_files/pytorch_model.bin'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfrom_pt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'outputs_training/model_files/tokenizer_config.json'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\proj\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1785\u001b[0m             \u001b[0muse_auth_token\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1786\u001b[0m             \u001b[0mcache_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1787\u001b[1;33m             \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1788\u001b[0m         )\n\u001b[0;32m   1789\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\proj\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[1;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1913\u001b[0m         \u001b[1;31m# Instantiate tokenizer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1914\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1915\u001b[1;33m             \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minit_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0minit_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1916\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1917\u001b[0m             raise OSError(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\proj\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, vocab_file, eos_token, unk_token, pad_token, extra_ids, additional_special_tokens, sp_model_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msp_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSentencePieceProcessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msp_model_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msp_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLoad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\proj\\lib\\site-packages\\sentencepiece\\__init__.py\u001b[0m in \u001b[0;36mLoad\u001b[1;34m(self, model_file, model_proto)\u001b[0m\n\u001b[0;32m    365\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mmodel_proto\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLoadFromSerializedProto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_proto\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 367\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLoadFromFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    368\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\proj\\lib\\site-packages\\sentencepiece\\__init__.py\u001b[0m in \u001b[0;36mLoadFromFile\u001b[1;34m(self, arg)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mLoadFromFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_sentencepiece\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSentencePieceProcessor_LoadFromFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mDecodeIdsWithCheck\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Internal: C:\\projects\\sentencepiece\\src\\sentencepiece_processor.cc(891) [model_proto->ParseFromArray(serialized.data(), serialized.size())] "
     ]
    }
   ],
   "source": [
    "#not sure if we even have to load in the tokenizes, is it even trained? idk\n",
    "tokenizer = T5Tokenizer.from_pretrained('outputs_training/model_files/pytorch_model.bin',from_pt=True,config='outputs_training/model_files/tokenizer_config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72351c70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
